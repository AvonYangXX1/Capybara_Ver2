---
title: "skeleton of 715 project"
output: html_document
date: "2023-12-09"
---
Author: Avon Yang, Nuoya Jiang
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Rubric 1: 
Statement of the research question:
Can we use predictor variables to achieve a high level of accuracy in identifying overweight individuals
1 dependent variable:
BMI, here we denote a y variable to represent overweight status based on bmi. (with a bmi > 24.5, we denote y as 1)

8 independent variable:
"RIDAGEYR", "RIAGENDR", "INDFMIN2","LBXSKSI", "LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI","BMXBMI"
Demographic: age, gender, income, Lab tests: blood potassium, cholesterol,protein, glucose, and sodium
Goal:
Classification of overweight individuals from non-overweight individuals

Rubric 2: EDA and QC of data
```{r}
subset_df <- read.csv("nhanes_13_14_subset.csv")

#age, gender, income, potassium, cholesterol, protein, serum glucose, blood sodium, bmi
data <- subset_df[, c("RIDAGEYR", "RIAGENDR", "INDFMIN2","LBXSKSI", "LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI","BMXBMI")]
#inspect if we have NA values
any_na <- any(is.na(data))
print(paste("Are there any NA values in the data frame? ", any_na))
#inspect num/cat.
str(data)
# Summary statistics for numeric columns
summary(data)

# TO DO: drop NA
data <- na.omit(data)

library(tidyr)
data <- na.omit(data)
str(data)
summary(data)
```
```{r}
num_variables <- ncol(data)
print(paste("Number of variables in the data frame: ", num_variables))
data_counts <- sapply(data, function(x) sum(!is.na(x)))
print(data_counts)
```
In the data cleaning process, we found our data contained many NA values, and to ensure the cleanness of data, we removed all records with NA values. Here we assume a "missingness completely at random."
```{r}
# EDA plots
library(ggplot2)
#install.packages("gridExtra")
library(gridExtra)

plots_list <- list()
# dot plot to gain insight and data trend from each independent variable and the original BMI dependent variable
numeric_vars <- names(data)[sapply(data, is.numeric) & names(data) != "BMXBMI"]
for (var in numeric_vars) {
    p <- ggplot(data, aes_string(x = var, y = "BMXBMI")) + 
        geom_point() +
        labs(x = var, y = "BMI") +
        theme_minimal()
    plots_list[[var]] <- p
}

do.call(grid.arrange, c(plots_list, ncol = 2))

```

```{r}
# Histograms to inspect distribution of indenependent variables from overweight and non-overweight individuals
library(dplyr)
data_overweight <- data %>% 
  filter(BMXBMI >= 24.9) %>% 
  mutate(BMI_Category = "BMI >= 24.9")
data_normal <- data %>% 
  filter(BMXBMI < 24.9) %>% 
  mutate(BMI_Category = "BMI < 24.9")
combined_data <- rbind(data_overweight, data_normal)

generate_histograms <- function(data) {
  plots_list <- list()
  vars <- names(data)[sapply(data, is.numeric)&names(data) != "BMXBMI"]
  for (var in vars) {
    p <- ggplot(data, aes_string(x = var, fill = "BMI_Category")) + 
      geom_histogram( alpha = 0.5, bins = 30) +
      scale_fill_manual(values = c("skyblue", "pink")) +
      labs(x = var, y = "Frequency", title = paste("Histogram of", var)) +
      theme_minimal()+
      theme(plot.title = element_text(size = 10)) 
    plots_list[[var]] <- p
  }
  return(plots_list)
}
combined_histograms <- generate_histograms(combined_data)
do.call(grid.arrange, c(combined_histograms, ncol = 2))
```

```{r}
# correlation and collinearity (Spearman)
# To ensure there is no significant correlation between independent variables
correlation_matrix <- cor(data[, -ncol(data)], method="spearman")
library(corrplot)
corrplot(correlation_matrix, method = "color", addCoef.col = "black", type = "upper", order = "hclust", tl.col="black", tl.srt=45)
```


```{r}
# scaling the predictor variables 
features <- data[setdiff(names(data), 'BMXBMI')]
target_variable <- data['BMXBMI']
features_numeric <- as.data.frame(lapply(features, function(x) if(is.factor(x)) as.numeric(as.character(x)) else x))
features_scaled <- scale(features_numeric)
data_scaled <- cbind(features_scaled, target_variable)

```

Rubric 3: 

Hypothesis test

Null hypothesis: there is no significant difference in mean BMI between the two genders.
Alternative hypothesis: there is a significant difference in mean BMI between the two genders.

t-test: 
assumptions: We do believe the sampling is random, and our sample is normal distributed

F-test:
P value of our F test is 8.21755e-28 which is significantly smaller than 0.05, so we reject the null hypothesis that the male and female have equal variances in bmi.

The t test p value is 8.745e-09 and is smaller than 0.05, so we reject the null hypothesis that there is no significant difference in mean BMI between the two genders.

```{r}
male_data <- data[data$RIAGENDR == 1, "BMXBMI"]
female_data <- data[data$RIAGENDR == 2, "BMXBMI"]
variance_test_result <- var.test(male_data, female_data)
print(variance_test_result$p.value)
t_test_result <- t.test(male_data, female_data)
print(t_test_result)
```

Rubric 4-5:
We chose decision tree as our model. Since there is no strong linear correlation between our dependent and independent variables, we select a non-parametric model: tree.
We discard income and potassium because of their minimal correlation coefficients (<0.1) with outcome variables.
```{r}
# add the y label (overweight status) to our dataset
data$y <- ifelse(data$BMXBMI <= 24.9, 0, 1)
set.seed(715)
#inspect correlation to make sure no collinearity and select variables based on correlation
cor_matrix<- cor(data)
print(cor_matrix)
#split data into train test split
split_index <- sample(1:nrow(data), 0.8 * nrow(data))

#Variable selection by not selecting INDFMIN2, LBXSKSI

X_train <- data[split_index, c("RIDAGEYR", "RIAGENDR", "LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI")]
y_train <- data[split_index, "y"]

X_test <- data[-split_index, c("RIDAGEYR", "RIAGENDR","LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI")]
y_test <- data[-split_index, "y"]
# inspect data imbalance
percentage_of_1 <- mean(y_train == 1) * 100
print(percentage_of_1)
percentage_of_1 <- mean(y_test == 1) * 100
print(percentage_of_1)
```


```{r}
library(tree)
tree_model <- tree(y_train ~ ., data = X_train)
```

Rubric 6: 
Evaluate model fit on our training dataset
To better cater to the goal of our model-correctly classify individuals, we chose to display our ROC curve, which shows all thresholds of our decision tree. 
```{r}
library(caret)
library(pROC)
predictions <- predict(tree_model, newdata = X_train)
# calculate raw accuracy as a refenrence
binary_predictions <- as.numeric(ifelse(predictions >= 0.5, 1, 0))
confusion_matrix <- confusionMatrix(factor(binary_predictions), factor(y_train))
print(confusion_matrix)
accuracy <- confusion_matrix$overall["Accuracy"]
print(paste("Accuracy:", accuracy))
# draw roc curves and calculate auc
roc_curve <- roc(y_train, as.numeric(predictions))
auc_value <- auc(roc_curve)
print(paste("AUC:", auc_value))
plot(roc_curve, main = "ROC Curve", col = "lightblue", lwd = 2)
```

Rubric 7:
To make another interesting model, we only select the demographic variables from our data set and test their accuracies for our train and test datasets. We are surprised to see that using only the demographic variables, we achieve a similar accuracy compared to the original model. This experiment shows the importance of demographic variables in another angle.
```{r}
X_train_d <- data[split_index, c("RIDAGEYR", "RIAGENDR", "INDFMIN2")]

X_test_d <- data[-split_index, c("RIDAGEYR", "RIAGENDR", "INDFMIN2")]

tree_model_d <- tree(y_train ~ ., data = X_train_d)
predictions_d <- predict(tree_model_d, newdata = X_train_d)

binary_predictions_d <- as.numeric(ifelse(predictions_d >= 0.5, 1, 0))
confusion_matrix_d <- confusionMatrix(factor(binary_predictions_d), factor(y_train))
accuracy_d <- confusion_matrix_d$overall["Accuracy"]
print(paste("Demographic Accuracy:", accuracy_d))

```

Rubric 8:
Conduct one follow-up analysis
Part 1: we test our accuracy, roc, and AUC on our test dataset and we observe no overfit in this case.
```{r}
predictions <- predict(tree_model, newdata = X_test)

binary_predictions <- as.numeric(ifelse(predictions >= 0.5, 1, 0))
confusion_matrix <- confusionMatrix(factor(binary_predictions), factor(y_test))
print(confusion_matrix)
accuracy <- confusion_matrix$overall["Accuracy"]
print(paste("Accuracy:", accuracy))

roc_curve <- roc(y_test, as.numeric(predictions))
auc_value <- auc(roc_curve)
print(paste("AUC:", auc_value))
plot(roc_curve, main = "ROC Curve", col = "lightblue", lwd = 2)
```

Part 2: we test our accuracy, roc, and AUC on a modified tree on our train and test datasets and we observe overfit in this case.
We add all variables and change the split criteria from the defalt deviance to gini index. The train score increases dramatically while the test score decreases slightly. The new model might not be generalized to new data with a low AUC and a low accuracy.
```{r}
# add all variables to the train test sets
X_train <- data[split_index, c("RIDAGEYR", "RIAGENDR", "INDFMIN2","LBXSKSI", "LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI")]

X_test <- data[-split_index, c("RIDAGEYR", "RIAGENDR", "INDFMIN2","LBXSKSI", "LBXSCH", "LBXSTP", "LBXSGL", "LBXSNASI")]

# A tree with gini as classification critera
tree_model_gini <- tree(y_train ~ ., data = X_train, split = "gini")
predictions_g <- predict(tree_model_gini, newdata = X_train)

binary_predictions_g <- as.numeric(ifelse(predictions_g >= 0.5, 1, 0))
confusion_matrix_g <- confusionMatrix(factor(binary_predictions_g), factor(y_train))
accuracy_g <- confusion_matrix_g$overall["Accuracy"]
print(paste("Train Accuracy:", accuracy_g))

roc_curve <- roc(y_train, as.numeric(predictions_g))
auc_value <- auc(roc_curve)
print(paste("Train AUC:", auc_value))

predictions_g <- predict(tree_model_gini, newdata = X_test)
binary_predictions_g <- as.numeric(ifelse(predictions_g >= 0.5, 1, 0))
confusion_matrix_g <- confusionMatrix(factor(binary_predictions_g), factor(y_test))
accuracy_g <- confusion_matrix_g$overall["Accuracy"]
print(paste("Test Accuracy:", accuracy_g))

roc_curve <- roc(y_test, as.numeric(predictions_g))
auc_value <- auc(roc_curve)
print(paste("Test AUC:", auc_value))
```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
